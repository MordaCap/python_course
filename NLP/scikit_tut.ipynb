{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with text data: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data bunch\n",
    "#twenty_train = sklearn.datasets.load_files('20news-bydate-train', categories=categories, encoding='latin1')\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 2257)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data), len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf[0].count_nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348868175765646"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9127829560585885"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "        alpha=1e-3, random_state=42,\n",
    "        max_iter=5, tol=None)),\n",
    "])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.81      0.87       319\n",
      "         comp.graphics       0.88      0.97      0.92       389\n",
      "               sci.med       0.94      0.90      0.92       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "           avg / total       0.92      0.91      0.91      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[258,  11,  15,  35],\n",
       "       [  4, 379,   3,   3],\n",
       "       [  5,  33, 355,   3],\n",
       "       [  5,  10,   4, 379]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(gs_clf.best_score_)\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\n",
      "Subject: help: Splitting a trimming region along a mesh \n",
      "Organization: University Of Kentucky, Dept. of Math Sciences\n",
      "Lines: 28\n",
      "\n",
      "\n",
      "\n",
      "\tHi,\n",
      "\n",
      "\tI have a problem, I hope some of the 'gurus' can help me solve.\n",
      "\n",
      "\tBackground of the problem:\n",
      "\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \n",
      "\tmapping of a 3d Bezier patch into 2d. The area in this domain\n",
      "\twhich is inside a trimming loop had to be rendered. The trimming\n",
      "\tloop is a set of 2d Bezier curve segments.\n",
      "\tFor the sake of notation: the mesh is made up of cells.\n",
      "\n",
      "\tMy problem is this :\n",
      "\tThe trimming area has to be split up into individual smaller\n",
      "\tcells bounded by the trimming curve segments. If a cell\n",
      "\tis wholly inside the area...then it is output as a whole ,\n",
      "\telse it is trivially rejected. \n",
      "\n",
      "\tDoes any body know how thiss can be done, or is there any algo. \n",
      "\tsomewhere for doing this.\n",
      "\n",
      "\tAny help would be appreciated.\n",
      "\n",
      "\tThanks, \n",
      "\tAni.\n",
      "-- \n",
      "To get irritated is human, to stay cool, divine.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.data[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. ==\n",
      "To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: ==\n",
      "Learning to filter netnews,” though he does not explicitly mention this collection. ==\n",
      "The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. ==\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent, '==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the DET DT det Xxx True False\n",
      "20 20 NUM CD compound dd False False\n",
      "Newsgroups newsgroups PROPN NNP compound Xxxxx True False\n",
      "data datum NOUN NNS compound xxxx True False\n",
      "set set VERB VBN nsubj xxx True False\n",
      "is be VERB VBZ ROOT xx True True\n",
      "a a DET DT det x True True\n",
      "collection collection NOUN NN attr xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "approximately approximately ADV RB advmod xxxx True False\n",
      "20,000 20,000 NUM CD nummod dd,ddd False False\n",
      "newsgroup newsgroup NOUN NN compound xxxx True False\n",
      "documents document NOUN NNS pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "partitioned partition VERB VBN acl xxxx True False\n",
      "( ( PUNCT -LRB- punct ( False False\n",
      "nearly nearly ADV RB advmod xxxx True False\n",
      ") ) PUNCT -RRB- punct ) False False\n",
      "evenly evenly ADV RB advmod xxxx True False\n",
      "across across ADP IN prep xxxx True True\n",
      "20 20 NUM CD nummod dd False False\n",
      "different different ADJ JJ amod xxxx True False\n",
      "newsgroups newsgroup NOUN NNS pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "for token in next(doc.sents):\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>The</td><td>the</td><td>DET</td><td>DT</td><td>det</td><td>Xxx</td><td>True</td><td>False</td></tr><tr><td>20</td><td>20</td><td>NUM</td><td>CD</td><td>nummod</td><td>dd</td><td>False</td><td>False</td></tr><tr><td>newsgroups</td><td>newsgroup</td><td>NOUN</td><td>NNS</td><td>compound</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>collection</td><td>collection</td><td>NOUN</td><td>NN</td><td>nsubj</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>has</td><td>have</td><td>VERB</td><td>VBZ</td><td>aux</td><td>xxx</td><td>True</td><td>True</td></tr><tr><td>become</td><td>become</td><td>VERB</td><td>VBN</td><td>ROOT</td><td>xxxx</td><td>True</td><td>True</td></tr><tr><td>a</td><td>a</td><td>DET</td><td>DT</td><td>det</td><td>x</td><td>True</td><td>True</td></tr><tr><td>popular</td><td>popular</td><td>ADJ</td><td>JJ</td><td>amod</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>data</td><td>data</td><td>NOUN</td><td>NN</td><td>attr</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>set</td><td>set</td><td>VERB</td><td>VBN</td><td>acl</td><td>xxx</td><td>True</td><td>False</td></tr><tr><td>for</td><td>for</td><td>ADP</td><td>IN</td><td>prep</td><td>xxx</td><td>True</td><td>True</td></tr><tr><td>experiments</td><td>experiment</td><td>NOUN</td><td>NNS</td><td>pobj</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>in</td><td>in</td><td>ADP</td><td>IN</td><td>prep</td><td>xx</td><td>True</td><td>True</td></tr><tr><td>text</td><td>text</td><td>NOUN</td><td>NN</td><td>compound</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>applications</td><td>application</td><td>NOUN</td><td>NNS</td><td>pobj</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>of</td><td>of</td><td>ADP</td><td>IN</td><td>prep</td><td>xx</td><td>True</td><td>True</td></tr><tr><td>machine</td><td>machine</td><td>NOUN</td><td>NN</td><td>compound</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>learning</td><td>learning</td><td>NOUN</td><td>NN</td><td>compound</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>techniques</td><td>technique</td><td>NOUN</td><td>NNS</td><td>pobj</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>,</td><td>,</td><td>PUNCT</td><td>,</td><td>punct</td><td>,</td><td>False</td><td>False</td></tr><tr><td>such</td><td>such</td><td>ADJ</td><td>JJ</td><td>amod</td><td>xxxx</td><td>True</td><td>True</td></tr><tr><td>as</td><td>as</td><td>ADP</td><td>IN</td><td>prep</td><td>xx</td><td>True</td><td>True</td></tr><tr><td>text</td><td>text</td><td>NOUN</td><td>NN</td><td>compound</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>classification</td><td>classification</td><td>NOUN</td><td>NN</td><td>pobj</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>and</td><td>and</td><td>CCONJ</td><td>CC</td><td>cc</td><td>xxx</td><td>True</td><td>True</td></tr><tr><td>text</td><td>text</td><td>NOUN</td><td>NN</td><td>compound</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>clustering</td><td>clustering</td><td>NOUN</td><td>NN</td><td>conj</td><td>xxxx</td><td>True</td><td>False</td></tr><tr><td>.</td><td>.</td><td>PUNCT</td><td>.</td><td>punct</td><td>.</td><td>False</td><td>False</td></tr></table>"
      ],
      "text/plain": [
       "<__main__.RenderTokens at 0x7fd359df26a0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "class RenderTokens:\n",
    "    def __init__(self, sent):\n",
    "        self.sent = sent\n",
    "    def _repr_html_(self):\n",
    "        builder = StringIO()\n",
    "        builder.write('<table>')\n",
    "        for token in sent:\n",
    "            builder.write('<tr><td>')\n",
    "            builder.write(\n",
    "                '</td><td>'.join(\n",
    "                    (token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                     token.shape_, str(token.is_alpha), str(token.is_stop))\n",
    "                )\n",
    "            )\n",
    "            builder.write('</td></tr>')\n",
    "        builder.write('</table>')\n",
    "        return builder.getvalue()\n",
    "\n",
    "RenderTokens(next(doc.sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат тестирования, который мы получили выше был основан на работе класса CountVectorizer с настройками по умолчанию, где он использовал в качестве токенов слова в той форме, в которой они встречались в тексте. То есть несколько форм одного слова - talk, talks, talked - являются разными словами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание: написато скрипт, чтобы посмотреть, как изменится качество, если для всех слов использовать лемматизацию (приведение к начальной форме слова). В качестве лемматизатора можно использовать библиотеку [spacy][1], как в примере выше (см. поля lemma и lemma_ токенов).\n",
    "\n",
    "[1]: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_docs36)",
   "language": "python",
   "name": "conda_docs36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
